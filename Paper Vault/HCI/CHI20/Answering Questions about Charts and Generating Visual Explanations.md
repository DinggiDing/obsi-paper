---
ì‹œì‘ì¼: 2024-10-15
ì§„í–‰ìƒíƒœ: false
tags:
  - HCI
  - Visualization
  - Explainable-AI
sticker: lucide//settings-2
Author: Dae hyun Kim
---
- They developed an automatic chart question answering pipeline that generates visual explanations describing how the answer was obtained

1. Extracts the data and visual encodings from **an input Vega-Lite chart**
2. Given a natural language question about the chart, it transforms references to visual attributes into **references to the data**
3. It applies **a machine learning algorithm** to answer the transformed question
4. It uses a template-based[^1] approach to explain in natural language **how the answer is determined from the chart's visual features**
[^1]: ë¯¸ë¦¬ ì •ì˜ëœ ë¬¸ì¥ êµ¬ì¡°ë‚˜ ë¬¸ì¥í˜•ì‹ì„ í™œìš©


# 1 Introduction
---
- Answering complex questions that require combining multiple visual attributes can be challenging
- They approach builds on Sempre, a question-answering system for relational data tables that focuses on answering compositional, non-visual questions
- *According to formative study,* they find that people frequestly ask compositional questions(70%)
	- Compositional question - comparing values and calculating sums... 
- the visual encoding structure of a chart is very useful for automatic chart question answering

# 3 Formative Study
---
- Lookup questions were often visual, while compositional question were more likely to be non-visual
- People frequestly ask compositional questions, and visual explanations were perceived as clearer

# 4 Method
---
### Stage 1 : Extract data table and encodings
- They Convert an input data into Vega-Lite specification and then extract encodings as well as the transformed data. 
- **Extract Encodings** : Extract encodings by looking for "encoding" keyword
- **Extract Data** : To get a flat relational table(such as single data tuple), they run the Vega-Lite interpreter and apply all data transformations in the chart specification
- **Unfold Data Table** : Table question answering systems like sempre are trained using human readable tables(unfolded format)

### Stage 2 : Visual to Non-Visual Question Conversation
- Why? - ì»´í“¨í„°ê°€ ë‹¨ìˆœíˆ ì‹œê°ì  íŠ¹ì§•ë§Œ ì´í•´í•˜ê¸° ì–´ë ¤ì›€, Sempre ì‹œìŠ¤í…œì€ ë¹„ì‹œê°ì  ë°ì´í„°ì— ìµœì í™”ë¨, ë³µë³µì¡í•œ ì—°ì‚°(ë¹„êµ, ë§ì…ˆ) ê°€ëŠ¥í•´ì§
- The system transforms questions referring to visual attributes into non-visual question(data-centric)
Ex) "*Which religion has the longest orange component?*" â†’  "*Which religion has the most Percentage of Common Response data?*"
- **1. Mark detection** : Detect all words referring to graphical marks in the chart. 
	- ex) component, portion and segment are included in the list for bar marks manually
- **2. Dependency parsing** : It helps identify relationships between words like "longest" and "bar" in question
- **3. Visual attribute detection** : Identify all the visual attribute words(such as color, length) in the list of descriptive words
- **4. Visual operation detection** : Interpret "longest" words as the visual operation(argmax)
- **5. Apply encodings** : Replace visual attributes with correspoding data fields ("color" â†’  "response: Common")
- **6. Natural language conversion**

### Stage 3 : Explanation Generation
- The lambda expression generated by Sempre is converted into a natural lnaguage explanation using templates
- **1. Natural language conversion** : our pipeline applies the argmax, type, lookup, and row rules to convert the input lambda expression to â€œâ€˜Religionâ€™ of data with the greatest â€˜Commonâ€™ of â€˜Religionâ€™.â€
- **2. Implicit field recovery** : Sometimes, a field name becomes implicit during the table unfolding in Stage 1
- **3. Redundancy cleanup** : our pipelines removes any redundant information using a series of regex rules
- **4. Sentence completion** : Generate a non-visual explanation by adding the pronoun 'I' and a verb
- **5. Encoding application** : Apply the visual encodings obtained from Stage 1

# 5 Result
---
- Comparing it with the baseline Sempre
- 51% Accuracy (Sempre : 39%)
- The pipeline excelled in visual questions with a 53% accuracy rate.

# 6 User Study
---
- to evaluate trust, transparency, usefulness
- 4 different conditions(`No explanation`, `Human explanation`, `Non-visual explanation`, `Visual explanation` )
	- H1 : visual explanation ğŸ‘ >  no explanation
		- visual explanations generated by our pipeline significantly increased the `transparency` of the pipeline compared to the no-explanation condition
	- H2 : Visual explanation ğŸ‘ >= Human explanation
		- our pipeline significantly more `transparent` than the human-generated explanations
	- H3 : Visual explanation ğŸ‘ > Non-visual explanation
		- none of the improvements are significant
- Participants were given questions about different charts ( using [[7-point likert scale]])

> In sum, we find the visual explanations generated by our system are significantly more `transparent` than human-generated explanations and are comparable in `usefulness` and `trust`.


# 7 Limitations
---
- Handling compositional quesion is difficult
- Limitations of the Sempre system
- Lack of flexibility in Template-baed explanations


##### vs Data Formulator ([[Data Formulator - AI-powered Concept-driven Visualization Authoring]])
- ì´ ë…¼ë¬¸ì€ ì°¨íŠ¸ì— ëŒ€í•œ ì§ˆë¬¸ê³¼ ë‹µë³€ ì •í™•ë„ë¥¼ ê°œì„ í•˜ëŠ”ë° ì´ˆì ì„ ë‘ , Data FormulatorëŠ” ë°ì´í„° ë³€í™˜ê³¼ ì°¨íŠ¸ ì„¤ê³„, ë°ì´í„° ì‹œê°í™” ìƒì„± ë° í¸ì§‘ì— ì´ˆì 
- ë‘ ë…¼ë¬¸ ë‹¤ AIë¥¼ í™œìš©í•˜ì—¬ ë°ì´í„°ì™€ ì°¨íŠ¸ì— ëŒ€í•œ ì´í•´ë¥¼ ë„ì›€.